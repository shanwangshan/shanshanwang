<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-11-11 Fri 22:34 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Shanshan Wang (王珊珊)</title>
<meta name="author" content="Shanshan Wang" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="imagine_dark.css" />
<base target="_blank">
</head>
<body>
<div id="content" class="content">
<h1 class="title">Shanshan Wang (王珊珊)</h1>
<p align="center">&emsp;&emsp;<a href="https://scholar.google.com/citations?user=K8aK11cAAAAJ&hl=en"><img src="google_scholar.png" width=30" height="50"></a>&nbsp;&nbsp; &nbsp; &nbsp; <a href="https://github.com/shanwangshan"><img src="github.png" width=30" height="50"></a>&nbsp;&nbsp; &nbsp; &nbsp; <a href="https://www.linkedin.com/in/wangshanshan/"><img src="linkedin.png" width=30" height="50"></a></p


<div id="orgeb04511" class="figure">
<p><a href="my_big.JPG" alt="image" style="float:left;margin:-60px 80px 10px -20px;" width="180px"><img src="my.JPG" alt="image" style="float:left;margin:-60px 80px 10px -20px;" width="180px" /></a>
</p>
</div>

<p>
<br />
<br />
</p>
<div id="outline-container-orgb363193" class="outline-2">
<h2 id="orgb363193">Hey (您好!)</h2>
<div class="outline-text-2" id="text-orgb363193">
<p>
I am a PhD student in <a href="https://research.tuni.fi/machinelistening/">Machine Listening</a> and <a href="https://webpages.tuni.fi/arg/">Audio Research Group</a>(ARG)
from <a href="https://www.tuni.fi/en">Tampere University</a>. I am working in collaboration with assistant
professor <a href="http://www.cs.tut.fi/~mesaros/index">Annamaria Mesaros</a> and professor <a href="http://www.cs.tut.fi/~tuomasv/">Tuomas Virtanen</a>. My research
interests include audio-visual feature learning, self-supervised
learning, audio signal processing, and general deep Learning. My
complete academic information can be found in <a href="https://drive.google.com/file/d/10ngKZmPPqbgQLynZbFd0n3aSuhEHTaWB/view">curriculum vitae.</a><br />
</p>
</div>
</div>

<div id="outline-container-org7eed057" class="outline-2">
<h2 id="org7eed057">Updates</h2>
<div class="outline-text-2" id="text-org7eed057">
<ul class="org-ul">
<li>I am actively looking for internships. Please feel free to contact
me if you think I am the right person you are looking for</li>
</ul>


<ul class="org-ul">
<li><i>23.10, 2022</i>, presented our recent journal work in ECCV2022
workshop <a href="https://av4d.org/">AV4D: Visual Learning of Sounds in Spaces</a></li>
</ul>


<ul class="org-ul">
<li>Served as a reviewer for <a href="https://dcase.community/challenge2022/index">DCASE2022</a>,  ECCV2022 workshop <a href="https://geometry.stanford.edu/voli/">VOLI</a></li>
</ul>


<ul class="org-ul">
<li><i>23.08 - 27.08, 2021</i>, selected as one of the ten finalists of the
<a href="https://eusipco2021.org/3-minute-thesis/">3 Minute Thesis (3MT)</a> contest, check my PhD thesis contest video
<a href="https://www.youtube.com/watch?v=GDbbfBA62t4&amp;t=13s&amp;ab_channel=ShanshanWang">Youtube link</a>.</li>
</ul>


<ul class="org-ul">
<li><i>01.03 - 01.07,2021</i>, one of the coordinators in DCASE2021 task1
subtaskb, <a href="http://dcase.community/challenge2021/task-acoustic-scene-classification">DCASE2021</a></li>
</ul>


<ul class="org-ul">
<li><i>02.2020 - present</i>, PhD student at Tampere University</li>
</ul>


<ul class="org-ul">
<li><i>04.2018 - 12.2019</i>, worked as research assistant in ARG</li>
</ul>


<ul class="org-ul">
<li><i>08.2017 - 12.2019</i>, master student at Tampere University</li>
</ul>


<ul class="org-ul">
<li><i>09.2013 - 06.2017</i>, bachelor student at China University of
Petroleum (East China)</li>
</ul>
</div>
</div>

<div id="outline-container-org42b63db" class="outline-2">
<h2 id="org42b63db">Publications</h2>
<div class="outline-text-2" id="text-org42b63db">
<ul class="org-ul">
<li><i>S. Wang, S. Tripathy, and A. Mesaros, "Self-supervised learning of</i>
<i>audio representations using angular contrastive loss", submitted</i>
<i>in 2023 IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP 2023):</i> <a href="https://github.com/shanwangshan/Self_supervised_ACL">Code</a>, <a href="https://arxiv.org/abs/2211.05442">Paper</a> .</li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, A. Politis, A. Mesaros and T. Virtanen, "Self-supervised</i>
<i>learning of audio representations from audio-visual data using
spatial alignment", in ECCV2022 workshop:</i> <a href="https://av4d.org/">Workshop link</a></li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, A. Politis, A. Mesaros and T. Virtanen, "Self-supervised</i>
<i>learning of audio representations from audio-visual data using</i>
<i>spatial alignment", in IEEE Journal of Selected Topics in Signal
Processing (JSTSP), 2022:</i> <a href="https://arxiv.org/abs/2206.00970">Paper</a></li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, A. Mesaros, T. Heittola and T. Virtanen, "Audio-visual</i>
<i>scene classification: analysis of DCASE 2021 Challenge
submissions", 2021:</i> <a href="https://arxiv.org/abs/2105.13675">Paper</a>, <a href="https://www.youtube.com/watch?v=NAJErjrghhE">Youtube link</a></li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, G. Naithani, A Politis, and T. Virtanen, "Deep neural</i>
<i>network Based Low-latency Speech Separation with Asymmetric
analysis-Synthesis Window Pair", in EUSIPCO, 2021:</i> <a href="https://arxiv.org/abs/2106.11794">Paper</a>, <a href="https://github.com/shanwangshan/asymmetric_window">Code</a>, <a href="https://youtu.be/ey_oPEN7L20">Youtube link</a>.</li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, A. Mesaros, T. Heittola and T. Virtanen, "A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis," in Proc. ICASSP, 2021:</i> <a href="https://arxiv.org/abs/2011.00030">Paper</a>, <a href="https://github.com/shanwangshan/TAU-urban-audio-visual-scenes">Code</a>, <a href="https://www.youtube.com/watch?v=89EwgWGXkCs&amp;t=61s&amp;ab_channel=ShanshanWang">Youtube link</a>.</li>
</ul>


<ul class="org-ul">
<li><i>S. Wang, G. Naithani, and T. Virtanen, “Low-latency deep
clustering for speech separation,” in Proc. ICASSP, 2019:</i> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683437">Paper</a>, <a href="https://github.com/shanwangshan/Low-latency_deep_clustering_for_speech_separation">Code</a>, <a href="https://www.youtube.com/watch?v=3tGHxScf6As&amp;t=1s&amp;ab_channel=ShanshanWang">Youtube link</a>.</li>
</ul>
</div>
</div>


<div id="outline-container-orge714a5f" class="outline-2">
<h2 id="orge714a5f">Master thesis</h2>
<div class="outline-text-2" id="text-orge714a5f">
<p>
Graduation: 2019, <a href="https://core.ac.uk/download/pdf/280342574.pdf">Thesis</a>
</p>
</div>
</div>

<div id="outline-container-orgabaf2ef" class="outline-2">
<h2 id="orgabaf2ef">Contacts</h2>
<div class="outline-text-2" id="text-orgabaf2ef">
<p>
Address: Tampere University, Hervanta Campus, Tietotalo, Room No: TE307<br />
Email: shanshan.wang@tuni.fi<br />
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Shanshan Wang</p>
<p class="date">Created: 2022-11-11 Fri 22:34</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
